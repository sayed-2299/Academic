{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8PgK1ZjjYoP"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 pandas tqdm newspaper3k\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/Project/Sarcasm_Headlines_Dataset_v2.json'\n",
        "valid_data = []\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            valid_data.append(json.loads(line.strip()))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Skipping invalid line: {line.strip()} (Error: {e})\")\n",
        "            continue\n",
        "\n",
        "df = pd.DataFrame(valid_data)\n",
        "print(\"Number of valid rows:\", len(df))\n",
        "print(df.head())\n",
        "print(\"Columns:\", df.columns)\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset not found\")\n",
        "\n",
        "def normalize_link(link):\n",
        "    link = re.sub(r'https?://(?:\\w+\\.)?theonion\\.com/', 'https://www.theonion.com/', link)\n",
        "    link = re.sub(r'https?://(?:\\w+\\.)?huffingtonpost\\.com/', 'https://www.huffpost.com/', link)\n",
        "    return link\n",
        "\n",
        "def fetch_full_article(original_link):\n",
        "    try:\n",
        "        link = normalize_link(original_link)\n",
        "        print(f\"Attempting to fetch article from: {link}\")\n",
        "        article = Article(link)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article_text = article.text.strip()\n",
        "\n",
        "        if not article_text:\n",
        "            raise ValueError(\"No text extracted from direct link\")\n",
        "\n",
        "        article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
        "        article_text = re.sub(r'[^\\w\\s.,!?]', '', article_text)\n",
        "\n",
        "        print(f\"Successfully fetched article from {link} (Length: {len(article_text)} characters)\")\n",
        "        return article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Direct fetch failed for {original_link}: {e}. Trying Wayback Machine...\")\n",
        "\n",
        "        try:\n",
        "            wayback_api = f\"http://archive.org/wayback/available?url={original_link}\"\n",
        "            response = requests.get(wayback_api, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if data.get('archived_snapshots', {}).get('closest', {}).get('available', False):\n",
        "                archived_url = data['archived_snapshots']['closest']['url']\n",
        "                print(f\"Attempting to fetch from Wayback Machine: {archived_url}\")\n",
        "                article = Article(archived_url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                article_text = article.text.strip()\n",
        "\n",
        "                if not article_text:\n",
        "                    raise ValueError(\"No text extracted from archive\")\n",
        "\n",
        "                # Preprocess\n",
        "                article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
        "                article_text = re.sub(r'[^\\w\\s.,!?]', '', article_text)\n",
        "\n",
        "                print(f\"Successfully fetched article from Wayback Machine for {original_link} (Length: {len(article_text)} characters)\")\n",
        "                return article_text\n",
        "            else:\n",
        "                print(f\"No archive available for {original_link}. Skipping...\")\n",
        "                return None\n",
        "        except Exception as archive_e:\n",
        "            print(f\"Wayback fetch failed for {original_link}: {archive_e}. Skipping...\")\n",
        "            return None\n",
        "\n",
        "def save_to_csv(data, output_path, mode='w'):\n",
        "    temp_df = pd.DataFrame(data)\n",
        "    temp_df.to_csv(output_path, index=False, encoding='utf-8', mode=mode)\n",
        "    print(f\"Saved {len(temp_df)} articles to {output_path}\")\n",
        "\n",
        "new_data = []\n",
        "success_count = 0\n",
        "batch_size = 500\n",
        "output_path = '/content/drive/My Drive/Colab Notebooks/Project/Extended_Sarcasm_Dataset.csv'\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    pd.DataFrame(columns=['headline', 'full_text', 'is_sarcastic', 'article_link']).to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing articles\"):\n",
        "    print(f\"\\nProcessing article {index + 1}/{len(df)}: {row['headline']}\")\n",
        "    full_text = fetch_full_article(row['article_link'])\n",
        "    if full_text:\n",
        "        new_data.append({\n",
        "            'headline': row['headline'],\n",
        "            'full_text': full_text,\n",
        "            'is_sarcastic': row['is_sarcastic'],\n",
        "            'article_link': row['article_link']\n",
        "        })\n",
        "        success_count += 1\n",
        "        print(f\"Added to dataset (Success count: {success_count})\")\n",
        "\n",
        "        if success_count % batch_size == 0:\n",
        "            save_to_csv(new_data, output_path, mode='a')\n",
        "            print(f\"Cleared batch data after saving {success_count} articles\")\n",
        "            new_data = []\n",
        "    else:\n",
        "        print(f\"Skipped article {index + 1} due to fetch failure\")\n",
        "    time.sleep(1)\n",
        "\n",
        "if new_data:\n",
        "    save_to_csv(new_data, output_path, mode='a')\n",
        "\n",
        "final_df = pd.read_csv(output_path)\n",
        "print(f\"Number of valid rows in final dataset: {len(final_df)}\")\n",
        "print(f\"Total articles successfully fetched: {success_count}/{len(df)}\")\n",
        "print(final_df.head())"
      ]
    }
  ]
}